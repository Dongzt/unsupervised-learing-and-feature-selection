# unsupervised-learing-and-feature-selection
# **无监督学习**
![项目证明](https://github.com/Dongzt/unsupervised-learing-and-feature-selection/assets/79237845/03ab87eb-6090-4008-8b66-fdecf1f4f02d)

### 定义

无监督学习（英语：unsupervised learning）是机器学习的一种方法，没有给定事先标记过的训练样本，自动对输入的数据进行分类或分群。

 ### 聚类

聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的类别。

#### K-means

K 均值聚类的步骤如下：

1. ==定义 K 个重心==。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）
2. 寻找最近的重心并且更新聚类分配。将每个数据点都分配给这 K 个聚类中的一个。==每个数据点都被分配给离它们最近的重心的聚类==。这里的「接近程度」的度量是一个超参数——通常是欧氏距离（Euclidean distance）。
3. ==将重心移动到它们的聚类的中心==。每个聚类的重心的新位置是通过计算该聚类中所有数据点的平均位置得到的。

重复第 2 和 3 步，直到每次迭代时重心的位置不再显著变化（即直到该算法收敛）

![image-20240208150015195](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240208150015195.png)

#### 高斯混合聚类

高斯混合模型（Gaussian Mixture Model, GMM）==是通过选择**最大化后验概率**来完成聚类的，各数据点的后验概率表示属于各类的可能性，而不是判定它完全属于某个类，所以称为软聚类==。其在各类尺寸不同、聚类间有相关关系的时候可能比k-means聚类更合适.

高斯混合聚类的步骤：
（1）首先，假设样本集具有一些规律，包括可以以**α**参数作为比例分为 **k** 类且每类内符合高斯分布。
（2）然后，根据贝叶斯原理利用极大似然法同时求出决定分类比例的**α**和决定类内高斯分布的**μ**、**Σ**。
（3）最后，将样本根据**α**、**μ**、**Σ**通过贝叶斯原理求出样本该分在哪个簇。
![image-20240208154000155](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240208154000155.png)

#### 层次聚类

层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。

AGNES是一种自底向上的层次聚类算法

![image-20240208154724847](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240208154724847.png)

#### 方法比较

K均值适用于大型数据集，其中簇的数量已知或者有明显的先验信息。它对于凸形簇的数据分布效果较好，对异常值敏感。

GMM适用于数据呈现出连续的、非凸形的分布，或者具有潜在的混合分布。它对于各种形状的簇以及重叠的簇效果较好。

层次聚类适用于数据中存在层次性结构的情况，或者当不清楚最佳聚类数量时。它对于小型数据集或者需要可视化展示聚类结果的情况较为合适。

**凸形分布**：当数据集的分布呈现出一种凸形状时，我们称之为凸形分布。具体来说，数据点相对密集地分布在一个中心点周围，并且随着距离中心点的增加而逐渐稀疏。在凸形分布中，数据点的分布通常比较均匀，而且簇之间的边界清晰明确。

#### 聚类效果评价指标

ACC(准确度) NMI(归一化互信息)

### 降维

在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”（curse ofdimensionality）。

缓解维数灾难的一个重要途径是降维（dimension reduction），亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间”（subspace），在这个子空间中样本密度大幅提高，距离计算也变得更为容易。

#### 主成分分析

PCA(Principal  Component  Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，==第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推==，可以得到n个这样的坐标轴。

通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。

![image-20240211125345839](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240211125345839.png)

#### 流形学习

流形（manifold）是几何中的一个概念，它是高维空间中的几何结构，即空间中的点构成的集合。可以简单的将流形理解成二维空间的曲线，三维空间的曲面在更高维空间的推广。

流形学习（manifold learning）==假设数据在高维空间的分布位于某一更低维的流形上==，基于这个假设来进行数据的分析。对于降维，==要保证降维之后的数据同样满足与高维空间流形有关的几何约束关系==。

**局部线性嵌入**（简称LLE）的核心思想是每个样本点都可以由与它相邻的多个点的线性组合（体现了局部线性）来近似重构，这相当于用分段的线性面片近似代替复杂的几何形状，样本投影到低维空间之后要保持这种线性重构关系，即有相同的重构系数。

LLE首先假设数据在较小的局部是线性的，也就是说，某一个数据可以由它邻域中的几个样本来线性表示。比如我们有一个样本x1,我们在它的原始高维邻域里用K-近邻思想找到和它最近的三个样本x2,x3,x4. 然后我们假设x1可以由x2,x3,x4线性表示，即：x1=w12x2+w13x3+w14x4　其中，w12，w13，w14为权重系数。

在我们通过LLE降维后，我们希望x1在低维空间对应的投影x′1和x2,x3,x4对应的投影x′2,x′3,x′4也尽量保持同样的线性关系，即x′1≈w12x′2+w13x′3+w14x′4 也就是说，投影前后线性关系的权重系数w12，w13，w14是尽量不变或者最小改变的。

从上面可以看出，线性关系只在样本的附近起作用，离样本远的样本对局部的线性关系没有影响，因此降维的复杂度降低了很多。

![image-20240211140359539](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240211140359539.png)

#### 方法比较

PCA适用于具有**线性结构**的数据，例如，数据集中的特征之间存在线性相关性的情况

流形学习适用于处理**非线性结构**的数据，例如，高维数据集中的样本分布在一个弯曲或扭曲的流形上，而不是在线性子空间中。

### 无监督特征选择

#### 过滤器

过滤式（ Filter）方法==先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关==，这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。

**Laplace Score** 是一个对一个训练集样本的特征进行打分的算法。通过这个算法可以给每一个特征打出一个分数，最后再取分数最低的k个特征作为最后选择的特征子集，是标准的Filter式方法。流程如下：

 1. 构建权重矩阵S

    ![image-20240212103203395](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240212103203395.png)

 2. 计算Laplacian Score

    ![](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/image-20240212103259577.png)

过滤的方法计算成本很低，但由于忽略了所选特征与归纳算法性能之间的关系，因此无法获得满意的性能。

#### 包装器

包裹式（Wrapper）特征选择直接把==最终将要使用的学习器的性能作为特征子集的评价准则==，换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。

从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。

#### 嵌入式

嵌入式（Embedded）特征选择是==将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成==，即在学习器训练过程中自动地进行了特征选择。

**L1正则化(Lasso)**将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此**L1正则化往往会使学到的模型很稀疏**（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

**L2正则化(Ridge Regression)**同样将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，**L2正则化会让系数的取值变得平均**。

### 算法复现

#### SPLR

**自定进度学习和低冗余正则化**（self-paced learning and low-redundant regularization）

**自定进度学习** 是基于curriculum learning发展而来的。相比于curriculum learning中需要将所有的样本人为的进行”难易”程度的排序，==self-paced learning更进一步的通过添加一个正则项(SP-regularizer），来实现模型自动挑选简单的，也就是易于分类的样本==。

课程学习：通过模拟人的认知机理，首先学习简单的、普适性的知识结构，然后逐渐增加难度，过渡到学习更复杂、更专业化的知识。

课程学习：根据先验知识赋予样本学习先后顺序

**自步学习**：学习算法在每一步迭代中决定下一步学习样本

样本的选择在自步学习中并不是随机的，或是在一次迭代中全部纳入训练过程中，而是通过一种由简到难的有意义的方式进行选择的。
– 简单样本可以理解为具有较小的损失（smaller loss）的样
本。
– 复杂的样本具有较大损失（larger loss）的样本。

==本研究结合自进度学习和子空间学习的框架，提出了一种无监督特征选择方法。此外，还保留了**局部流形结构**，并通过两个正则化项约束了特征的冗余性==。投影矩阵采用**L2,1/2-范数**，利用l2,1/2范数来保证矩阵的行稀疏性,既保留了判别特征，又进一步减轻了数据中噪声的影响。

![image-20240304100840445](https://picbed-983735321.oss-cn-beijing.aliyuncs.com/imgimage-20240304100840445.png)



# 
